databaseChangeLog:
  - changeSet:
      id: create cs_entity table
      author: epyataev
      changes:
        - createTable:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  type: bigint
                  autoIncrement: true
                  constraints:
                    primaryKey: true
              - column:
                  name: name
                  type: nvarchar(255)
                  constraints:
                    nullable: false
              - column:
                  name: description
                  type: text
              - column:
                  name: parent_id
                  type: bigint
                  constraints:
                    foreignKeyName: cs_entity__parent__fk
                    references: cs_entity(id)

  - changeSet:
      id: add cs_entities
      author: epyataev
      changes:
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 1
              - column:
                  name: name
                  value: SISD
              - column:
                  name: description
                  value: SISD (single instruction stream / single data stream) - одиночный поток команд и одиночный поток данных. К этому классу относятся, прежде всего, классические последовательные машины, или иначе, машины фон-неймановского типа, например, PDP-11 или VAX 11/780. В таких машинах есть только один поток команд, все команды обрабатываются последовательно друг за другом и каждая команда инициирует одну операцию с одним потоком данных. Не имеет значения тот факт, что для увеличения скорости обработки команд и скорости выполнения арифметических операций может применяться конвейерная обработка - как машина CDC 6600 со скалярными функциональными устройствами, так и CDC 7600 с конвейерными попадают в этот класс.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 2
              - column:
                  name: name
                  value: SIMD
              - column:
                  name: description
                  value: SIMD (single instruction stream / multiple data stream) - одиночный поток команд и множественный поток данных. В архитектурах подобного рода сохраняется один поток команд, включающий, в отличие от предыдущего класса, векторные команды. Это позволяет выполнять одну арифметическую операцию сразу над многими данными - элементами вектора. Способ выполнения векторных операций не оговаривается, поэтому обработка элементов вектора может производится либо процессорной матрицей, как в ILLIAC IV, либо с помощью конвейера, как, например, в машине CRAY-1.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 3
              - column:
                  name: name
                  value: MISD
              - column:
                  name: description
                  value: MISD (multiple instruction stream / single data stream) - множественный поток команд и одиночный поток данных. Определение подразумевает наличие в архитектуре многих процессоров, обрабатывающих один и тот же поток данных. Однако ни Флинн, ни другие специалисты в области архитектуры компьютеров до сих пор не смогли представить убедительный пример реально существующей ВС, построенной на данном принципе. Ряд исследователей (например, Хендлер) относят конвейерные машины к данному классу, однако это не нашло окончательного признания в научном сообществе. Считается, что пока данный класс пуст.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 4
              - column:
                  name: name
                  value: MIMD
              - column:
                  name: description
                  value: MIMD (multiple instruction stream / multiple data stream) - множественный поток команд и множественный поток данных. Этот класс предполагает, что в ВС есть несколько устройств обработки команд, объединенных в единый комплекс и работающих каждое со своим потоком команд и данных.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 5
              - column:
                  name: name
                  value: С единственным функциональным устройством
              - column:
                  name: description
                  value: Дополнение Ванга и Бриггса к классификации Флинна к классу SISD. Пример - PDP-11.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 6
              - column:
                  name: name
                  value: Имеющие несколько функциональных устройств
              - column:
                  name: description
                  value: Дополнение Ванга и Бриггса к классификации Флинна к классу SISD. Пример - CDC 6600, CRAY-1, FPS AP-120B, CDC Cyber 205, FACOM VP-200.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 7
              - column:
                  name: name
                  value: C пословно-последовательной обработкой информации
              - column:
                  name: description
                  value: Дополнение Ванга и Бриггса к классификации Флинна к классу SIMD. Примеры - ILLIAC IV, PEPE, BSP.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 8
              - column:
                  name: name
                  value: С разрядно-последовательной обработкой
              - column:
                  name: description
                  value: Дополнение Ванга и Бриггса к классификации Флинна к классу SIMD. Примеры - STARAN, ICL DAP.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 9
              - column:
                  name: name
                  value: Со слабой связью между процессорами
              - column:
                  name: description
                  value: Дополнение Ванга и Бриггса к классификации Флинна к классу MIMD, к которым они относят все системы с распределенной памятью, например, Cosmic Cube.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 10
              - column:
                  name: name
                  value: С сильной связью между процессорами
              - column:
                  name: description
                  value: Дополнение Ванга и Бриггса к классификации Флинна к классу MIMD, куда попадают такие компьютеры, как C.mmp, BBN Butterfly, CRAY Y-MP, Denelcor HEP.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 11
              - column:
                  name: name
                  value: PDP-11
              - column:
                  name: description
                  value: PDP-11 — серия 16-разрядных мини-ЭВМ компании DEC, серийно производившихся и продававшихся в 1970—80-х годах. Развитие серии PDP-8 из общей линейки компьютеров PDP. В PDP-11 появилось несколько уникальных технологических инноваций, эта серия была проще в программировании, чем её предшественники. Но, несмотря на её всеобщее признание со стороны программистов, PDP-11 со временем была вытеснена персональными компьютерами, включая IBM PC и Apple II.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 12
              - column:
                  name: name
                  value: CDC 6600
              - column:
                  name: description
                  value: CDC 6600 — первый в мире суперкомпьютер, разработанный и созданный американской компанией Control Data Corporation в 1963 году под руководством талантливого инженера-электронщика Сеймура Крэя, названного впоследствии «отцом суперкомпьютеров».
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 13
              - column:
                  name: name
                  value: FPS AP-120B
              - column:
                  name: description
                  value: The FPS AP-120B was a 38-bit, pipeline-oriented array processor manufactured by Floating Point Systems. It was designed to be attached to a host computer such as a DEC PDP-11 as a fast number-cruncher. Data transfer was accomplished using direct memory access. Processor cycle time was 167 nanoseconds, giving a speed of 6 MHz. Since it could present two floating point results per cycle, one from the adder and the other from the multiplier, a capacity of 12 Megaflops was claimed for the processor.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 14
              - column:
                  name: name
                  value: CDC Cyber 205
              - column:
                  name: description
                  value: The CDC Cyber 205 range of mainframe-class supercomputers were the primary products of Control Data Corporation (CDC) during the 1970s and 1980s. In their day, they were the computer architecture of choice for scientific and mathematically intensive computing. They were used for modeling fluid flow, material science stress analysis, electrochemical machining analysis, probabilistic analysis, energy and academic computing, radiation shielding modeling, and other applications. The lineup also included the Cyber 18 and Cyber 1000 minicomputers.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 15
              - column:
                  name: name
                  value: FACOM VP-200
              - column:
                  name: description
                  value: A series of supercomputers from Fujitsu. The vector processor FACOM VP-100 and FACOM VP-200 were announced as the company's first supercomputers in July 1982. These were developed, by exploiting the technology of the FACOM 230-75 APU, with the aim of achieving even higher performance, ease-of-use and affinity with the FACOM M Series of general-purpose computers.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 16
              - column:
                  name: name
                  value: CRAY-1
              - column:
                  name: description
                  value: Cray-1 — легендарный суперкомпьютер, спроектированный Сеймуром Крэем и созданный компанией Cray Research Inc. в 1976 году. Пиковая производительность машины — 133 Мфлопса. Cray-1 — это первый суперкомпьютер компании Cray Research, основанной «отцом суперкомпьютеров» Сеймуром Крэем после его ухода из компании CDC. Cray-1 — легендарный суперкомпьютер, спроектированный Сеймуром Крэем и созданный компанией Cray Research Inc. в 1976 году. Пиковая производительность машины — 133 Мфлопса. Cray-1 — это первый суперкомпьютер компании Cray Research, основанной «отцом суперкомпьютеров» Сеймуром Крэем после его ухода из компании CDC.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 17
              - column:
                  name: name
                  value: ILLIAC IV
              - column:
                  name: description
                  value: he ILLIAC IV was the first massively parallel computer. The system was originally designed to have 256 64-bit floating point units (FPUs) and four central processing units (CPUs) able to process 1 billion operations per second. Due to budget constraints, only a single "quadrant" with 64 FPUs and a single CPU was built. Since the FPUs all had to process the same instruction – ADD, SUB etc. – in modern terminology the design would be considered to be single instruction, multiple data, or SIMD.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 18
              - column:
                  name: name
                  value: PEPE
              - column:
                  name: description
                  value: The Parallel Element Processing Ensemble (PEPE) was one of the very early parallel computing systems. Bell began researching the concept in the mid-1960s as a way to provide high-performance computing support for the needs of anti-ballistic missile (ABM) systems. The goal was to build a computer system that could simultaneously track hundreds of incoming ballistic missile warheads. A single PEPE system was built by Burroughs Corporation in the 1970s, by which time the US Army's ABM efforts were winding down. The design later evolved into the Burroughs Scientific Computer for commercial sales, but a lack of sales prospects led to it being withdrawn from the market.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 19
              - column:
                  name: name
                  value: BSP
              - column:
                  name: description
                  value: The BSP model was developed by Leslie Valiant of Harvard University during the 1980s. The definitive article was published in 1990. Between 1990 and 1992, Leslie Valiant and Bill McColl of Oxford University worked on ideas for a distributed memory BSP programming model, in Princeton and at Harvard. Between 1992 and 1997, McColl led a large research team at Oxford that developed various BSP programming libraries, languages and tools, and also numerous massively parallel BSP algorithms. With interest and momentum growing, McColl then led a group from Oxford, Harvard, Florida, Princeton, Bell Labs, Columbia and Utrecht that developed and published the BSPlib Standard for BSP programming in 1996. Valiant developed an extension to the BSP model in the 2000s, leading to the publication of the Multi-BSP model in 2011. In 2017, McColl developed a major new extension of the BSP model that provides fault tolerance and tail tolerance for large-scale parallel computations in AI, Analytics and HPC
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 20
              - column:
                  name: name
                  value: STARAN
              - column:
                  name: description
                  value: STARAN might be the first commercially available computer designed around an associative memory. The STARAN computer was designed and built by Goodyear Aerospace Corporation. It is a Content Addressable Parallel Processor (CAPP), a type of parallel processor which uses content addressable memory. STARAN is a single instruction, multiple data array processor with a 4x256 1-bit processing element (PE) computer. The STARAN machines became available in 1972. Goodyear Aerospace later developed the MPP based on similar principles but with a larger and wider processor array.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 21
              - column:
                  name: name
                  value: ICL DAP
              - column:
                  name: description
                  value: The Distributed Array Processor (DAP) produced by International Computers Limited (ICL) was the world's first commercial massively parallel computer. The original paper study was complete in 1972 and building of the prototype began in 1974. The first machine was delivered to Queen Mary College in 1979.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 22
              - column:
                  name: name
                  value: Cosmic Cube
              - column:
                  name: description
                  value: The Caltech Cosmic Cube was a parallel computer, developed by Charles Seitz and Geoffrey C Fox from 1981 onward. It was the first working hypercube built. It was an early attempt to capitalise on VLSI to speed up scientific calculations at a reasonable cost. Using commodity hardware and an architecture suited to the specific task (QCD), Fox and Seitz demonstrated that this was indeed possible. In 1984 a group at Intel including Justin Rattner and Cleve Moler developed the Intel iPSC inspired by the Cosmic Cube. In 1987 several people in the group formed a company called Parasoft to commercialize the message passing interface developed for the Cosmic Cube.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 23
              - column:
                  name: name
                  value: C.mmp
              - column:
                  name: description
                  value: The C.mmp was an early multiple instruction, multiple data (MIMD) multiprocessor system developed at Carnegie Mellon University (CMU) by William Wulf (1971). The notation C.mmp came from the PMS notation of Gordon Bell and Allen Newell, where a central processing unit (CPU) was designated as C, a variant was noted by the dot notation, and mmp stood for Multi-Mini-Processor. As of 2020, the machine is on display at CMU, in Wean Hall, on the floor nine.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 24
              - column:
                  name: name
                  value: BBN Butterfly
              - column:
                  name: description
                  value: BBN Butterfly — параллельный суперкомпьютер, построенный компанией Bolt, Beranek and Newman (англ.)русск. в 1980-х годах. Название Butterfly (с англ. «бабочка») он получил из-за использованной топологии сетевого соединения butterfly switch между узлами компьютера. Число процессоров в BBN Butterfly могло достигать 512.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 25
              - column:
                  name: name
                  value: CRAY Y-MP
              - column:
                  name: description
                  value: Cray Y-MP — параллельный векторный многопроцессорный суперкомпьютер (PVP/SMP) производства компании Cray Research, созданный в 1988 году как преемник предыдущей модели — X-MP. Большой вклад в разработку Y-MP внес ведущий инженер компании Cray Research Стив Чен до своего ухода из компании в сентябре 1987 года.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 26
              - column:
                  name: name
                  value: Denelcor HEP
              - column:
                  name: description
                  value: The Heterogeneous Element Processor (HEP) was introduced by Denelcor, Inc. in 1982. The HEP's architect was Burton Smith. The machine was designed to solve fluid dynamics problems for the Ballistic Research Laboratory.[1] A HEP system, as the name implies, was pieced together from many heterogeneous components -- processors, data memory modules, and I/O modules. The components were connected via a switched network. A single processor, called a PEM, in a HEP system (up to sixteen PEMs could be connected) was rather unconventional; via a "program status word (PSW) queue," up to fifty processes could be maintained in hardware at once. The largest system ever delivered had 4 PEMs. The eight-stage instruction pipeline allowed instructions from eight different processes to proceed at once. In fact, only one instruction from a given process was allowed to be present in the pipeline at any point in time. Therefore, the full processor throughput of 10 MIPS could only be achieved when eight or more processes were active; no single process could achieve throughput greater than 1.25 MIPS. This type of multithreading processing classifies the HEP as a barrel processor. The hardware implementation of the HEP PEM was emitter-coupled logic.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 27
              - column:
                  name: name
                  value: Ассоциативные вычислительные системы
              - column:
                  name: description
                  value: Ассоциативные вычислительные системы (ABC) - системы класса SIMD, в которых процессоры параллельно и однотипно обрабатывают множественные данные. Их основное отличие заключается в том, что обращение к данным производится по отличительным признакам, содержащимся в самих данных или в приданных этим данным дополнительных разрядах. Как и в матричных системах, в ABC также множество процессорных элементов (ПЭ) параллельно по одной команде однотипно обрабатывают множественные данные. Однако в отличие от матричных систем обращение к данным производится не по адресам, где хранятся эти данные, а по отличительным признакам, содержащимся в самих данных или в приданных этим данным дополнительных разрядах. Такая возможность обеспечивается ассоциативными процессорами (АП), на базе которых строятся ABC.
                    Ассоциативные процессоры
                    Ассоциативным процессором называют специализированный процессор, реализованный на базе ассоциативного запоминающего устройства (АЗУ), где, как известно, доступ к информации осуществляется не по адресу операнда, а по отличительным признакам, содержащимся в самом операнде. От АЗУ традиционного применения ассоциативный процессор (АП) отличают две особенности. наличие средств обработки данных и возможность параллельной записи во все ячейки, для которых было зафиксировано совпадение с ассоциативным признаком. Последнее свойство АП известно как мультизапись.
                    Способ выполнения операций над словами позволяет определить четыре класса ассоциативных процессоров
                    параллельные;
                    поразрядно-последовательные;
                    пословно-последовательные;
                    блочно-ориентированные.
                    Два последних класса не слишком перспективны для универсальных вычислений, и ориентированы в основном на задачи информационного поиска.
                    В качестве элементов обработки в параллельной ассоциативном процессоре используются многоразрядные процессорные элементы. Каждый ПЭ, работает со своим модулем ассоциативной памяти, и осуществляет поиск, а также арифметическую и логическую обработку m-разрядных слов. Пересылку выбранных по содержанию слов между АЗУ и ПЭ обеспечивают коммутирующие цепи. Процессорные элементы одновременно выполняют одну и ту же команду, поступающую из процессора управления. Кроме того, предусмотрена возможность обмена данными между модулями ассоциативной памяти и основной памятью, причем обращения по этому каналу производятся как и в обычной памяти - по адресам.
                    Параллельные АП по сравнению с другими классами ассоциативных процессоров обладают наиболее высоким быстродействием, однако это достигается за счет больших аппаратурных затрат.
                    Поразрядно-последовательные ассоциативные процессоры в настоящее время являются наиболее распространенными. Запоминающий массив ассоциативного процессора обычно представляет собой матрицу n×n 1-разрядных запоминающих элементов (ЗЭ). Считывание и запись информации могут производиться по двум срезам запоминающего массива - либо это все разряды одного слова, либо один и тот же разряд всех слов. Каждый разряд среза в АП снабжен собственным одноразрядным процессорным элементом, что позволяет между считыванием информации и ее записью производить необходимую обработку, то есть параллельно выполнять операции арифметического сложения, поиска, а также эмулировать многие черты матричных ВС, таких, например, как ILLIAC IV. Все ПЭ одновременно выполняют одну и ту же команду, поступающую из процессора управления. АП оперирует m (m ≤ n) разрядами среза, причем значение m устанавливается программистом. При необходимости выделения отдельных разрядов среза лишние позиции допустимо маскировать.
                    Входящий в ПЭ операционный блок представляет собой одноразрядное арифметико-логическое устройство с цепью переноса из младшего разряда в старший. Промежуточные результаты могут быть временно сохранены в регистре. Ассоциативный доступ к разрядам среза, выборку и сохранение данных в ассоциативной памяти, а также связь с процессором управления, откуда поступают команды для ПЭ, обеспечивает коммутирующая схема. Триггер маски используется для блокирования процессорного элемента. При нулевом его состоянии ПЭ не выполняет команду, поступившую одновременно на все процессорные элементы ассоциативного процессора. В целом, ПЭ реализует арифметические, логические, а также по¬исковые и системные операции.
                    Конструктивно поразрядно-последовательный ассоциативный процессор исполняется в виде матрицы, содержащей m процессорных элементов и ассоциативную память емкостью n×n битов.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 28
              - column:
                  name: name
                  value: Векторные вычислительные системы
              - column:
                  name: description
                  value: Векторные вычислительные системы - системы класса SIMD, в которых одна и та же заданная операция выполняется сразу над всеми компонентами векторов.
                    В задачах моделирования реальных процессов и объектов, для которых характерна обработка больших массивов чисел в форме с плавающей запятой, массивы представляются матрицами и векторами, а алгоритмы их обработки описываются в терминах матричных операций. Как известно, основные матричные операции сводятся к однотипным действиям над парами элементов исходных матриц, которые, чаще всего, можно производить параллельно. В универсальных вычислительных системах, ориентированных на скалярные операции, обработка матриц выполняется поэлементно и последовательно. При большой размерности массивов последовательная обработка элементов матриц занимает слишком много времени, что и приводит к неэффективности универсальных ВС для рассматриваемого класса задач. Для обработки массивов требуются вычислительные средства, позволяющие с помощью единой команды производить действие сразу над всеми элементами массивов - средства векторной обработки.
                    В средствах векторной обработки под вектором понимается одномерный массив данных (обычно в форме с плавающей запятой), размещенных в памяти ВС. Количество элементов массива называется длиной вектора. Многомерные массивы считаются наборами одномерных массивов-векторов.
                    Действия над многомерными массивами учитывают специфику их размещения. Способ размещения многомерного массива влияет на шаг изменения адреса элемента, выбираемого из памяти. Так, если матрица расположена в памяти построчно, адреса соседних элементов строки различаются на единицу, а для элементов столбца шаг равен четырем. При размещении матрицы по столбцам единице будет равен шаг по столбцу, а четырем - шаг по строке. В векторной концепции для обозначения шага, с которым элементы вектора извлекаются из памяти, применяют термин шаг по индексу (stride).
                    Понятие векторного процессора
                    Векторный процессор - это процессор, в котором операндами некоторых команд могут выступать массивы данных - векторы. Векторный процессор может быть реализован в двух вариантах. В первом он представляет собой дополнительный блок к универсальной вычислительной машине (системе). Во втором - векторный процессор является основой самостоятельной ВС.
                    В архитектуре средств векторной обработки используется один из двух подходов - векторно-параллельный или векторно-конвейерный.
                    В векторно-параллельном процессоре одновременные операции над элементами векторов проводятся с помощью нескольких функциональных блоков (ФБ) с плавающей запятой, каждый из которых отвечает за одну пару элементов.
                    В векторно-конвейерном варианте обработка элементов векторов производится одним конвейерным ФБ. Операции с числами в форме с ПЗ достаточно сложны, но поддаются разбиению на отдельные шаги. Каждый этап обработки может быть реализован с помощью отдельной ступени конвейерного ФБ. Очередная пара элементов векторов-операндов подается на вход конвейера как только освобождается его первая ступень.
                    Одновременные операции над элементами векторов можно проводить и с помощью нескольких конвейерных ФБ. Такого рода обработка совмещает векторно-параллельный и векторно-конвейерный подходы.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 29
              - column:
                  name: name
                  value: CYBER
              - column:
                  name: description
                  value: Семейство систем CYBER разрабатывалось Control Data Corporation (CDC) в течение 1970-1980 годов. Системы использовались для научных, математических вычислений.
                    Эволюция архитектуры STAR-100 привела к созданию семейства конвейерных систем CYBER-203 (1979) и CYBER-205 (1981).
                    Архитектура этих ВС не изменялась в процессе развития, т.е. была типа «память-память». Производительность системы CYBER-203 (или STAR-100A, как она первоначально называлась) также оставалась 100 млн опер./с. Эту систему можно было рассматривать как модернизированный вариант STAR-100, она была конвейерной, имела ту же систему команд и полностью совместимое программное обеспечение. Однако в отличие от STAR-100 система CYBER-203 содержала обычный скалярный процессор (вместо конвейера K3), который обеспечивал шестикратное увеличение быстродействия при скалярной обработке информации. Емкость оперативной памяти CYBER-203 была увеличена до 16 Мбайт, скорость выборки из памяти - до 100 млрд бод (разрядность слов - 64). Элементную базу системы CYBER-203 составляли БИС.
                    Система CYBER-205 обладала более совершенной архитектурой в сравнении с CYBER-203. Так, в ней допускалось варьирование числа конвейеров (с изменяемой конфигурацией) от одного до четырех. Пиковая производительность ВС CYBER-205 достигала 200 млн опер./с, емкость оперативной памяти - 32 Мбайт.
                    Однако все конвейеры CYBER-205 могли работать только в унисон, т.е. все они могли выполнять одновременно только одну и ту же векторную операцию (а не несколько различных). Следовательно, архитектура CYBER-205 в целом представляла собой архитектуру SIMD.
                    В составе аппаратурно реализованных векторных операций CYBER-205 имелись также триады A + αB, где А и В - векторы; α - скаляр; αВ - вектор, получаемый из А путем умножения его компонентов на число α.
                    Система CYBER-205 могла выполнять триады почти с такой же скоростью, как отыскание суммы или произведения векторов.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 30
              - column:
                  name: name
                  value: STAR-100
              - column:
                  name: description
                  value: Разработка конвейерной системы STAR-100 (STAR - STring ARray computer - векторный компьютер) осуществлялась фирмой CDC с 1965 по 1973 г. Система была анонсирована в 1970 г., а первая ее поставка была проведена в августе 1973 г. Быстродействие ВС - 108 опер./с, стоимость - 15 млн долл.
                    Система STAR-100 создавалась с учетом языка программирования APL (A Programming Language). Язык APL (или АЛЛ) - диалоговый язык программирования, характеризуется развитыми средствами работы с регулярными структурами данных (векторами, матрицами, массивами) и богатым набором базовых операций и компактностью записи.
                    Вычислительная система STAR-100 состояла из двух подсистем. первая осуществляла переработку данных, вторая - функции операционной системы. Ядром первой подсистемы являлся процессор, образуемый из нескольких конвейеров. В типовых конфигурациях системы STAR-100 процессоры состояли из трех конвейеров. K1 K2, K3. Конвейеры были специализированными. два из них (K1, K2) служили для выполнения векторных операций, а третий (K3) - для реализации операций над скалярными операндами, т.е. K1 и K2 - конвейеры (Floating-Point Pair Pipelines), каждый из которых служил для выполнения операций с плавающей запятой над парами векторов данных, K3 - конвейер (string data pipeline), предназначавшийся для обработки обычных операндов, не организованных в векторы. Конвейеры K1 и K2 выполняли основной объем вычислений, следовательно, они определяли уровень быстродействия системы STAR-100 в целом.
                    Конвейеры STAR-100 имели программируемую структуру (т.е. были с изменяемой конфигурацией), следовательно, в них можно было (на одном и том же множестве элементарных блоков обработки) выполнять различные арифметические операции. Однако, до начала новой операции конвейер следовало перенастроить (запрограммировать на выполнение очередной операции).
                    В конвейерах K1 и K2 путем введения служебного булевского вектора была обеспечена избирательная обработка компонентов векторов-операндов. Единица в i-м разряде булевского вектора означала, что операция над i-ми компонентами соответствующей пары векторов производиться не будет.
                    В каждом конвейере была заложена возможность реализации операции сложения, а в двух из них - K1 и K2 - операций умножения и деления. Состав элементарных блоков обработки информации конвейеров был выбран с учетом распределения вероятностей использования микроопераций различных типов.
                    Каждый конвейер Ki (i = 1, 2, 3) мог включать в себя приблизительно 30 блоков обработки информации. Все блоки работали параллельно, но каждый из них оперировал с вполне определенными элементами векторов данных либо со своими скалярными операндами.
                    Любой конвейер воспринимал 64-разрядный код либо как один 64-разрядный операнд, либо как два 32-разрядных операнда. Время выполнения операции над парой операндов в любом из блоков конвейеров не превышало 40 нс. Следовательно, данные могли поступать в процессор (точнее, только в конвейеры K1 и K2) со скоростью 103 млн опер./с.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 31
              - column:
                  name: name
                  value: Матричные вычислительные системы
              - column:
                  name: description
                  value: Матричные вычислительные системы - наиболее распространенные представители класса SIMD, лучше всего приспособленные для решения задач, характеризующихся параллелизмом независимых данных. Матричная система состоит из множества процессорных элементов, работающих параллельно и обрабатывающих свой поток данных.
                    Назначение матричных вычислительных систем - обработка больших массивов данных. В основе матричных систем лежит матричный процессор (array processor), состоящий из массива процессорных элементов (ПЭ). Такие системы имеют общее управляющее устройство, генерирующее поток команд, и большое число ПЭ, работающих параллельно и обрабатывающих каждый свой поток данных. Однако на практике, чтобы обеспечить достаточную эффективность системы при решении широкого круга задач, необходимо организовать связи между процессорными элементами так, чтобы наиболее полно загрузить процессоры работой. Именно характер связей между ПЭ и определяет разные свойства системы. Подобная схема применима и для векторных вычислений.
                    Между матричными и векторными системами есть существенная разница. Матричный процессор интегрирует множество идентичных функциональных блоков (ФБ), логически объединенных в матрицу и работающих в SIMD-стиле. Не столь существенно, как конструктивно реализована матрица процессорных элементов - на едином кристалле или на нескольких. Важен сам принцип - ФБ логически скомпонованы в матрицу и работают синхронно, то есть присутствует только один поток команд для всех. Векторный процессор имеет встроенные команды для обработки векторов данных, что позволяет эффективно загрузить конвейер из функциональных блоков. В свою очередь, векторные процессоры проще использовать, потому что команды для обработки векторов - это более удобная для человека модель программирования, чем SIMD.
                    Параллельную обработку множественных элементов данных обеспечивает массив процессорных элементов (МПЭ). Единый поток команд, управляющий обработкой данных в МПЭ, генерируется контроллером массива процессорных элементов (КМП). КМП выполняет последовательный программный код, реализует операции условного и безусловного переходов, транслирует в МПЭ команды, данные и сигналы управления. Команды обрабатываются процессорными элементами в режиме жесткой синхронизации. Сигналы управления используются для синхронизации команд и пересылок, а также для управления процессом вычислений, в частности определяют, какие ПЭ массива должны выполнять операцию, а какие - нет. Команды, данные и сигналы управления передаются из КМП в массив процессорных элементов по шине широковещательной рассылки. Поскольку выполнение операций условного перехода зависит от результатов вычислений, результаты обработки данных в массиве процессоров транслируются в КМП по шине результата.
                    Для обеспечения пользователя удобным интерфейсом при создании и отладке программ в состав подобных ВС обычно включают фронтальную ВМ (front-end computer). В роли такой ВМ выступает универсальная вычислительная машина, на которую дополнительно возлагается задача загрузки программ и данных в КМП. Кроме того, такая загрузка может производиться и напрямую с устройств ввода/вывода, например с магнитных дисков. После загрузки КМП приступает к выполнению программы, транслируя в МПЭ по широковещательной шине соответствующие SIMD-команды.
                    Рассматривая массив ПЭ, следует учитывать, что для хранения множественных наборов данных в нем, помимо множества процессорных элементов, должно присутствовать и множество модулей памяти. Кроме того, в массиве должна быть реализована сеть взаимосвязей, как между ПЭ, так и между процессорными элементами и модулями памяти. Таким образом, под термином массив процессорных элементов понимают блок, состоящий из собственно процессорных элементов, модулей памяти и сети соединений.
                    Дополнительную гибкость при работе с рассматриваемой системой обеспечивает механизм маскирования, позволяющий вовлекать в операции лишь определенное подмножество ПЭ массива. Маскирование возможно как на стадии компиляции, так и на этапе выполнения, при этом ПЭ, исключенные путем установки в ноль соответствующих битов маски, во время выполнения команды простаивают.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 32
              - column:
                  name: name
                  value: Вычислительные системы с систолической структурой
              - column:
                  name: description
                  value: Систолические вычислительные системы - системы класса SIMD, основным принципом которых является то, что все данные регулярно и ритмически проходящие через массив, используются многократно. Это позволяет значительно повысить эффективность и достичь высокой вычислительной производительности за счет распараллеливания вычислений и сокращения обмена систолической системы с внешними устройствами.
                    В фон-неймановских машинах данные, считанные из памяти, однократно обрабатываются в процессорном элементе (ПЭ), после чего снова возвращаются в память. Авторы идеи систолической матрицы Кунг и Лейзерсон предложили организовать вычисления так, чтобы данные на своем пути от считывания из памяти до возвращения обратно пропускались через как можно большее число ПЭ.
                    Если проводить аналогию между структурами ВС и живого организма, то памяти можно отвести роль сердца, множеству ПЭ - роль тканей, а поток данных следует рассматривать как циркулирующую кровь. Отсюда и происходит название систолическая матрица (систола - сокращение предсердий и желудочков сердца при котором кровь нагнетается в артерии). Систолические структуры эффективны при выполнении матричных вычислений, обработке сигналов, сортировке данных и т.д.
                    В качестве примера авторами идеи был предложен линейный массив для алгоритма матричного умножения. В основе схемы лежит ритмическое прохождение двух потоков данных навстречу друг другу. Последовательные элементы каждого потока разделены одним тактовым периодом, чтобы любой из них мог пересечься с любым элементом встречного потока. Вычисления выполняются параллельно в процессорных элементах, каждый из которых реализует один шаг в операции вычисления скалярного произведения (IPS, Inner Product Step) и носит название IPS- элемента.
                    Таким образом, систолическая структура - это однородная вычислительная среда из процессорных элементов, совмещающая в себе свойства конвейерной и матричной обработки и обладающая следующими особенностями.
                    вычислительный процесс в систолических структурах представляет собой непрерывную и регулярную передачу данных от одного ПЭ к другому без запоминания промежуточных результатов вычисления;
                    каждый элемент входных данных выбирается из памяти однократно и используется столько раз, сколько необходимо по алгоритму, ввод данных осуществляется в крайние ПЭ матрицы;
                    образующие систолическую структуру ПЭ однотипны и каждый из них может быть менее универсальным, чем процессоры обычных многопроцессорных систем. Тип ПЭ выбирается в соответствии с назначением систолической матрицы и структурой пространственных связей (наиболее распространены процессорные элементы, ориентированные на умножение с накоплением);
                    потоки данных и управляющих сигналов обладают регулярностью, что позволяет объединять ПЭ локальными связями минимальной длины;
                    алгоритмы функционирования позволяют совместить параллелизм с конвейерной обработкой данных;
                    производительность матрицы можно улучшить за счет добавления в нее определенного числа ПЭ, причем коэффициент повышения производительности при этом линеен.
                    В настоящее время достигнута производительность систолических процессоров порядка 1000 млрд операций/с.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 33
              - column:
                  name: name
                  value: Вычислительные системы на базе транспьютеров
              - column:
                  name: description
                  value: Транспьютер - это сверхбольшая интегральная микросхема, заключающая в себе центральный процессор, блок операций с плавающей запятой, статическое оперативное запоминающее устройство, интерфейс с внешней памятью и несколько каналов связи.
                    Появление транспьютеров связано с идеей создания различных по производительности ВС (от небольших до мощных массивно- параллельных) посредством прямого соединения однотипных процессорных чипов. Сам термин объединяет два понятия - «транзистор» и «компьютер».
                    Одна из важнейших отличительных черт транспьютера - каналы связи (линки), благодаря которым транспьютеры можно объединять, создавая вычислительные системы с различной вычислительной мощностью.
                    Канал связи состоит из двух последовательных линий для двухстороннего обмена. Он позволяет объединить транспьютеры между собой и обеспечить взаимные коммуникации. Одна из последовательных линий используется для пересылки данных, а вторая - подтверждений. Передача информации производится синхронно под воздействием либо общего генератора тактовых импульсов (ГТИ), либо локальных ГТИ с одинаковой частотой следования импульсов. Информация передается в виде пакетов. Каждый раз, когда пересылается пакет данных, приемник отвечает пакетом подтверждения.
                    Пакет данных состоит из двух битов-единиц, за которыми следуют 8-битовые данные и ноль (всего 11 битов). Пакет подтверждения - это простая комбинация 10 (всего два бита), она может быть передана, как только пакет данных будет идентифицирован интерфейсом входного канала.
                    На базе транспьютеров легко могут быть построены различные виды ВС. Так, четыре канала связи обеспечивают построение двухмерного массива, где каждый транспьютер связан с четырьмя ближайшими соседями. Возможны и другие конфигурации, например объединение транспьютеров в группы с последующим соединением групп между собой. Если группа состоит из двух транспьютеров, для подключения ее к другим группам свободными остаются шесть каналов связи. Комплекс из трех транспьютеров также оставляет свободными шесть каналов, а для связи с «квартетом» транспьютеров остаются еще четыре канала связи. Группа из пяти транспьютеров может иметь полный набор взаимосвязей, но за счет потери возможности подключения к другим группам. Наконец, транспьютерную систему можно создать на основе кроссбара.
                    Особенности транспьютеров потребовали разработки для них специального языка программирования Occam. Название языка связано с именем философа-схоласта четырнадцатого века Оккама - автора концепции «бритвы Оккама». «entia non sunt multiplicanda praeter necessitatem» - «понятия не должны усложняться без необходимости». Язык обеспечивает описание простых операций пересылки данных между двумя точками, а также позволяет явно указать на параллелизм при выполнении программы несколькими транспьютерами. Основным понятием программы на языке Occam является процесс, состоящий из одного или более операторов программы, которые могут быть выполнены последовательно или параллельно. Процессы могут быть распределены по транспьютерам вычислительной системы, при этом оборудование транспьютера поддерживает совместное использование транспьютера одним или несколькими процессами.
                    Транспьютеры успешно использовались в различных областях от встроенных систем до суперЭВМ вплоть до 90-х годов прошлого века. Интересные возможности, связанные с построением вычислительных систем без привлечения дополнительного оборудования, оказали ощутимое влияние на архитектурные идеи матричных, систолических, SMP и МРР вычислительных систем. Однако появление универсальных микропроцессоров, также обладающих возможностью соединения их в систему, привело к прекращению производства транспьютеров и их вытеснению похожими разработками ведущих фирм, которые уже не позиционируются как транспьютеры.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 34
              - column:
                  name: name
                  value: Вычислительные системы с неоднородным доступом к памяти
              - column:
                  name: description
                  value: Вычислительные системы с неоднородным доступом к памяти - система, состоящая из однородных узлов, состоящих из процессора и блока памяти и объединенных с помощью высокоскоростного коммутатора. Здесь поддерживается единое адресное пространство, аппаратно поддерживается доступ к удаленной памяти, т.е. к памяти других модулей. При этом доступ к локальной памяти в несколько раз быстрее, чем к удаленной.
                    Примеры систем, с архитектурой NUMA. HP 9000 V-class в SCA-конфигурациях, SGI Origin2000, Sun HPC 10000, IBM/Sequent NUMA-Q 2000, SNI RM600.
                    В вычислительных системах с неоднородным доступом к памяти реализована технология NUMA (Non-Uniform Memory Access).
                    Технология неоднородного доступа к памяти считается одной из путей создания крупномасштабных вычислительных систем. В архитектуре NUMA память физически распределена, но логически общедоступна. Это позволяет сохранить преимущества архитектуры с единым адресным пространством, а также ощутимо расширяет возможности масштабирования ВС.
                    В типичной организации системы типа ccNUMA имеется множество независимых составляющих ВС (узлов), объединенных с помощью какой-либо сети соединений (например, кроссбара, кольца и т.д.). Узел содержит процессор с кэш-памятью, а также локальную основную память, рассматриваемую как часть глобальной основной памяти системы.
                    Согласно технологии неоднородного доступа, каждый узел в системе владеет локальной памятью, но с позиций системы имеет место глобальное адресное пространство, где каждая ячейка любой локальной основной памяти имеет уникальный системный адрес. Когда процессор инициирует доступ к памяти и нужная ячейка отсутствует в его локальной кэш- памяти, организуется операция выборки. Если нужная ячейка находится в локальной памяти, выборка производится с использованием локальной шины. Если же требуемая ячейка хранится в удаленной секции глобальной памяти (локальной памяти другого процессора), то автоматически формируется запрос, посылаемый по сети соединений на локальную шину узла, где находится запрошенная информация, и уже по ней на подключенную к данной локальной шине кэш-память. Все эти действия выполняются автоматически, прозрачны для процессора и его кэш-памяти.
                    Как и в любой ВС с разделяемой памятью, особое внимание уделяется когерентности кэшей. В подавляющем большинстве NUMA-систем реализована аппаратная поддержка когерентности кэш-памяти процессорных элементов (ccNUMA), хотя известны системы, где такая поддержка отсутствует (nccNUMA). Поскольку для взаимодействия узлов системы используется не шина, а сеть соединений с более сложной топологией, в ccNUMA-системах проблема когерентности решается с помощью протоколов на основе распределенных справочников. Хотя отдельные реализации и отличаются в деталях, общим является то, что каждый узел содержит справочник. Взаимодействуя между собой, справочники позволяют определить физическое расположение любой информации в глобальном адресном пространстве.
                    В реальных NUMA-системах узлы обычно содержат не одиночные процессорные элементы, а сборки из нескольких ПЭ, чаще всего - SMP-системы. Так, одна из наиболее производительных ВС - Tera 10 - состоит из 544 SMP-узлов, каждый из которых содержит от 8 до 16 процессоров Itanium 2.
                    NUMA-системы, как правило, работают под управлением единой операционной системы.
                    Масштабируемость NUMA-систем ограничена лишь величиной адресного пространства, возможностями аппаратных средств поддержки когерентности кэшей и возможностями операционной системы по управлению большим числом процессоров. Например, NUMA-система Silicon Graphics Origin поддерживает до 1024 процессоров R10000, а система Sequent NUMA-Q объединяет 252 процессора Pentium II. Очередным этапом развития технологии NUMA стала архитектура NumaFlex, используемая в семействе SGI 3000, где допускается наращивание системы даже за счет различных процессоров.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 35
              - column:
                  name: name
                  value: Вычислительные системы с программируемой структурой
              - column:
                  name: description
                  value: Вычислительные системы с программируемой структурой полностью основываются на модели коллектива вычислителей и являются композицией взаимосвязанных элементарных машин (ЭМ). Каждая ЭМ в своем составе обязательно имеет локальный коммутатор (ЛК), процессор и память; может иметь также внешние устройства. Локальная память ЭМ предназначается для хранения и части данных, и, главное, ветви параллельной программы. Архитектура ВС с программируемой структурой относится к типу MIMD.
                    Такие ВС прежде всего ориентированы на распределенную обработку информации; они эффективны и при конвейерной, и при матричной обработке. При распределенном способе обработки данных на ВС полностью используются возможности MIMD-архитектуры. При конвейерном и матричном способах обработки данных архитектура MIMD виртуально трансформируется соответственно в архитектуру MISD и SIMD. Системы с программируемой структурой рассчитываются на работу во всех основных режимах. решения сложной задачи, обработки наборов задач, обслуживания потоков задач, реализации функций вычислительной сети.
                    Концепция ВС с программируемой структурой была сформулирована в Сибирском отделении АН СССР, первая система («Минск-222») была построена в 1965-1966 гг.
                    Этапы исследований
                    Работы в области высокопроизводительных средств обработки информации выполнялись в Институте математики (ИМ) СО АН СССР в 1960-х годах под руководством специалиста по вычислительной технике Э.В. Евреинова (1928, профессор с 1972 г.). Первая работа сотрудников ИМ СО АН СССР [2] о возможности построения ВС высокой производительности опередила американские публикации в данной области примерно на 6 месяцев. В середине 1960-х годов вышла в свет монография [3], обобщающая первые результаты работ ИМ СО АН СССР по функциональным структурам ВС и параллельному программированию. Во второй половине 1960-х годов были созданы и первые ВС. «Минск-222» (1965-1966) и управляющая ВС для автоматизации научных исследований (1964-1967). К началу 1970-х годов завершилось формирование концепции ВС с программируемой структурой, как средств обработки информации, основанных на модели коллектива вычислителей. Первоначальное название рассматриваемых средств - «Однородные вычислительные системы» [2]; в конце 1970-х годов закрепляется название «ВС с программируемой структурой» (см. [4], стр. 26), так как оно точнее отражает архитектурные возможности систем - коллективов вычислителей. «Однородные ВС» и «ВС с программируемой структурой» следует рассматривать как синонимические термины.
                    Начиная с 70-х годов XX в. теоретические и проектные работы в Сибирском отделении АН СССР (ныне СО РАН) по ВС с программируемой структурой велись под руководством одного из разработчиков первой ВС с программируемой структурой «Минск-222» В.Г. Хорошевского (1940; д-р техн. наук с 1974 г., чл.-кор. РАН с 2000 г.).
                    Работы по ВС из академической сферы распространяются в промышленность, под руководством Хорошевского создается ряд систем. МИНИМАКС (1975), СУММА (1976), МИКРОС-1 (1986), МИКРОС-2 (1992), МИКРОС-Т (1996). Выходит в свет большое число публикаций как сотрудников СО АН СССР (СО РАН), так и других организаций [1, 2, 3, 4, 5, 6].
                    В 90-х годах XX в. активизируются работы по построению отечественных промышленных ВС с массовым параллелизмом. Работы по созданию систем семейства МВС (генераций МВС-100 и МВС-1000) выполнены в кооперации научно-исследовательских институтов РАН и промышленности.
                    Вычислительные масштабируемые системы генерации МВС-100 эксплуатируются с 1992 г. Количество процессоров в различных конфигурациях составляет от 4 до 128; производительность конфигураций ВС - от 400 MFLOPS до 10 GFLOPS. Системы генерации МВС-1000 поставляются с 1998 г.; их большемасштабные конфигурации ВС позволяют достичь производительности 10...103 GFLOPS.
                    Вычислительные системы с программируемой структурой - это распределенные средства обработки информации. В таких ВС нет единого функционально и конструктивно реализованного устройства. все компоненты (устройство управления, процессор и память) являются распределенными. Тип архитектуры ВС - MIMD; в системах заложена возможность программной перенастройки архитектуры MIMD в архитектуры MISD или SIMD.
                    Основной функционально-структурной единицей вычислительных ресурсов в системах рассматриваемого класса является элементарная машина (ЭМ). Допускается конфигурирование ВС с произвольным числом ЭМ. Следовательно, ВС с программируемой структурой относятся к масштабируемым средствам обработки информации и допускают формирование конфигураций с массовым параллелизмом.
                    Вычислительные системы с программируемой структурой сочетают в себе достоинства универсальных и специализированных средств обработки информации, в них допускается автоматическое формирование виртуальных проблемно-ориентированных конфигураций. В таких ВС с достаточной полнотой воплощены перспективные архитектурные принципы, системы основаны на модели коллектива вычислителей, обладают большими потенциальными возможностями по обеспеченно высоких значений показателей эффективности функционирования.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 36
              - column:
                  name: name
                  value: АСТРА
              - column:
                  name: description
                  value: АСТРА - первая территориально-распределенная вычислительная система с программируемой структурой*.
                    Эта система создана ИМ СО АН СССР и Новосибирским электротехническим институтом MB и ССО РСФСР; работы по проектированию ВС были начаты в 1970 г., а первая ее конфигурация была сдана в эксплуатацию в 1972 г. Среди конфигураций ВС АСТРА имелись городские и междугородные (Новосибирск - Москва). Они формировались из средств ЭВМ «Минск-32» и использовали телефонные каналы связи. Были выполнены проекты распределенных ВС и на базе машин третьего поколения семейства ЕС ЭВМ.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 37
              - column:
                  name: name
                  value: Минск-222
              - column:
                  name: description
                  value: «Минск-222» - первая в мире вычислительная система с программируемой структурой.
                    Интерес к практической реализации ВС с программируемой структурой постоянно проявлялся, начиная с 60-х годов XX в. Первоначально он поддерживался прежде всего необходимостью проверки теоретических основ построения ВС, необходимостью отработки архитектурных решений и функциональной структуры ВС, а также параллельных вычислительных технологий. Позднее возрастающую роль стал играть утилитарный компонент целей создания ВС, в 1970-х годах этот компонент стал превалировать над исследовательским. Последнее обосновывается потребностью в ВС, обладающих и высокой производительностью, надежностью и живучестью.
                    Работы по построению ВС, основанных на принципах коллектива вычислителей, были инициированы в Институте математики (ИМ) Сибирского отделения АН СССР в 1964 г. Вскоре в ИМ СО АН СССР было организовано и мини-производство ВС.
                    В проекте «Минск-222» были отработаны архитектурные, технические и программные решения, значительная часть из которых была «канонизирована» разработчиками не-фон-неймановских вычислительных средств.
                    Система «Минск-222» была разработана и построена Отделением вычислительной техники ИМ СО АН СССР совместно с Конструкторским бюро завода им. Г.К. Орджоникидзе Министерства радио-промышленности СССР (г. Минск). Руководитель работ по созданию ВС «Минск-222» - Э.В. Евреинов; основные разработчики. В.Г. Хорошевский, Б.А. Сидристый, Г.П. Лопато, А.Н. Василевский. Работы по проектированию ВС «Минск-222» были начаты в 1965 г., а первый ее образец был установлен в апреле 1966 г. в Институте математики АН БССР. Системы «Минск-222» были смонтированы в нескольких организациях Советского Союза и эксплуатировались более 15 лет.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 38
              - column:
                  name: name
                  value: МИНИМАКС
              - column:
                  name: description
                  value: МИНИМАКС - система с программируемой структурой, созданная Институтом математики СО АН СССР и Научно-производственным объединением «Импульс» Министерства приборостроения, средств автоматизации и систем управления СССР в 1975 году.
                    МИНИМАшинная программно Коммутируемая Система (МИНИМАКС) была создана Институтом математики СО АН СССР (Отделом вычислительных систем) и Научно-производственным объединением «Импульс» Министерства приборостроения, средств автоматизации и систем управления СССР (г. Северодонецк). Технический проект МИНИМАКС разработан в 1974 г., а опытно-промышленный образец системы был изготовлен и отработан в 1975 г.
                    Архитектура системы МИНИМАКС.
                    MIMD-архитектура;
                    распределенность средств управления, обработки и памяти;
                    параллелизм, однородность, модульность;
                    программируемость структуры;
                    двумерная (циркулянтная) топология;
                    масштабируемость;
                    живучесть;
                    максимальное использование промышленных средств мини-ЭВМ.
                    Функциональная структура мини-ВС МИНИМАКС
                    Функциональная структура мини-ВС МИНИМАКС - композиция из произвольного количества элементарных машин и программно настраиваемой сети связей между ними.
                    Системные взаимодействия, осуществлявшиеся в мини-ВС, были следующих типов.
                    программное изменение структуры мини-ВС и степени участия ЭМ в системных взаимодействиях (настройка);
                    передача информации из оперативного запоминающего устройства одной ЭМ в памяти других (обмен);
                    выполнение ОБП;
                    синхронизация работы ЭМ;
                    реализация ОУП по значению признака Ω.
                    Межмашинные взаимодействия при функционировании мини-ВС реализовывались с помощью специальных подпрограмм - системных драйверов, которые, в свою очередь, использовали специальные команды (занесение кода на регистр настройки, считывание его содержимого, занесение информации в СУ о начальном адресе передаваемого массива данных и т.п.).
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 39
              - column:
                  name: name
                  value: МИКРОС
              - column:
                  name: description
                  value: Система с программируемой структурой МИКРОС была разработана Отделом вычислительных систем СО АН СССР (СО РАН) в содружестве с подразделениями Научно-производственного объединения «Алмаз» и Научно-исследовательского института «Квант» Министерства радиопромышленности СССР в начале 1980-х.
                    В начале 1980-х в Отделе вычислительных систем Сибирского отделения АН СССР проводились работы по научно-исследовательскому проекту МИКРОС*, целью которых было создание Микропроцессорных Систем с программируемой структу-рой (МИКРОС). Результатом работ явилось семейство МИКРОС, включающее модели МИКРОС-1 (1986); МИКРОС-2 (1992); МИКРОС-Т (1996). Разработка моделей семейства МИКРОС осуществлялась Отделом вычислительных систем СО АН СССР (СО РАН) в содружестве с подразделениями Научно-производственного объединения «Алмаз» и Научно-исследовательского института «Квант» Министерства радиопромышленности СССР (г. Москва).
                    Архитектура систем семейства МИКРОС.
                    MIMD-архитектура;
                    распределенность средств управления, обработки и памяти;
                    массовый параллелизм (при обработке данных и управлении процессами);
                    программируемость структуры сети межмашинных связей;
                    возможность программной трансформации MIMD-архитектуры в SIMD и MISD;
                    децентрализация ресурсов;
                    асинхронность и близко действие;
                    масштабируемость, модульность и однородность.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 40
              - column:
                  name: name
                  value: МВС
              - column:
                  name: description
                  value: Вычислительные системы семейства МВС (МВС-100 и МВС-1000) созданы Научно-исследовательским институтом «Квант» (Москва) в содружестве с институтами РАН. Руководителем работ являлся В.К. Левин (1929; академик РАН с 2002 г.). Системы МВС вобрали в себя отечественные и зарубежные достижения в области архитектуры ВС и производства микропроцессорных БИС.
                    Современная элементная база индустрии обработки информации - это большие интегральные схемы, среди которых выделяются. микропроцессоры и микросхемы памяти (статические и динамические). В 1980-х и 1990-х годах электронная промышленность освоила производство высокопроизводительных микропроцессоров и микросхем памяти большой емкостью. Особенностью рынка БИС в те годы было огромное разнообразие универсальных микропроцессоров, а также сигнальных и медийных микропроцессоров.Заметной вехой в вычислительной индустрии стала одна из разработок фирмы Inmos (Великобритания), именно - транспьютер (Transputer). Под транспьютером понимается микропроцессор с собственной внутренней памятью и коммуникационными каналами (линками) для соединения с другими транспьютерами. Первый транспьютер - Т414 - был разработан фирмой Inmos в 1983 г.; он выпускался серийно с 1983 г. и имел следующие технические характеристики. разрядность - 32 двоичных разряда; тактовая частота - 20 МГц; быстродействие - 10 MIPS; объем внутренней памяти - 2 К байт; число линков - 4; скорость передачи информации по линку - 5, 10, 20 Мбит/с.
                    Транспьютер являлся простейшим вариантом ЭМ; он служил для реализации не только вычислительных, но и коммуникационных функций. В 1980-х годах транспьютер был эффективным функционально-конструктивным элементом для построения ВС с массовым параллелизмом. Однако в 1990-х годах транспьютер использовался лишь только в качестве коммуникационного элемента для построения ВС.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 41
              - column:
                  name: name
                  value: Кластерные вычислительные системы
              - column:
                  name: description
                  value: Кластер - группа взаимно соединенных вычислительных систем (узлов), работающих совместно и составляющих единый вычислительный ресурс, создавая иллюзию наличия единственной ВМ. Для связи узлов используется одна из стандартных сетевых технологий (Fast/Gigabit Ethernet, Myrinet) на базе шинной архитектуры или коммутатора. Примеры кластерных вычислительных систем. NT-кластер в NCSA, Beowulf-кластеры.
                    Одно из самых современных направлений в области создания вычислительных систем - кластеризация. Помимо термина «кластерные вычисления», достаточно часто применяют такие названия. кластер рабочих станций (workstation cluster), гипервычисления (hypercomputing), параллельные вычисления на базе сети (network-based concurrent computing), ультравычисления (ultracomputing).
                    Изначально перед кластерами ставились две задачи. достичь большой вычислительной мощности и обеспечить повышенную надежность ВС. Пионером в области кластерных архитектур считается корпорация DEC, разработавшая первый коммерческий кластер в начале 80-х годов прошлого века.
                    Архитектура кластерных систем во многом похожа на архитектуру МРР-систем.
                    Тот же принцип распределенной памяти, использование в качестве вычислительных узлов законченных вычислительных машин, большой потенциал для масштабирования системы и целый ряд других особенностей. В первом приближении кластерную технологию можно рассматривать как развитие идей массовых параллельных вычислений. С другой стороны, многие черты кластерной архитектуры дают основание считать ее самостоятельным направлением в области MIMD-систем.
                    В качестве узла кластера может выступать как однопроцессорная ВМ, так и ВС типа SMP (логически SMP-система представляется как единственная ВМ). Как правило, это не специализированные устройства, приспособленные под использование в вычислительной системе, как в МРР, а серийно выпускаемые вычислительные машины и системы. Еще одна особенность кластерной архитектуры состоит в том, что в единую систему объединяются узлы разного типа, от персональных компьютеров до мощных ВС. Кластерные системы с одинаковыми узлами называют гомогенными кластерами, а с разнотипными узлами - гетерогенными кластерами.
                    Использование машин массового производства существенно снижает стоимость ВС, а возможность варьирования различных по типу узлов позволяет получить необходимую производительность за приемлемую цену. Важно и то, что узлы могут функционировать самостоятельно и отдельно от кластера. Для этого каждый узел работает под управлением своей операционной системы. Чаще всего используются стандартные ОС. Linux, FreeBSD, Solaris и версии Windows, продолжающие на-правление Windows NT.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 42
              - column:
                  name: name
                  value: Параллельные векторные системы
              - column:
                  name: description
                  value: Параллельные векторные системы (PVP, Parallel Vector Processor) - MIMD-системы с однородным доступом к разделяемой памяти. Основным признаком PVP-систем является наличие специальных векторно-конвейерных процессоров, в которых предусмотрены команды однотипной обработки векторов независимых данных, эффективно выполняющиеся на конвейерных функциональных устройствах. Примеры таких систем NEC SX-4/SX-5, линия векторно-конвейерных компьютеров CRAY от CRAY-1, CRAY J90/T90, CRAY SV1, CRAY X1, серия Fujitsu VPP.
                    По своей сути PVP-системs - это SMP-системы, где роль процессорных элементов исполняют векторно-конвейерные процессоры.
                    PVP-система содержит сравнительно небольшое число индивидуальных векторных процессоров, связанных широкополосным коммутатором типа «кроссбар». Благодаря кроссбару количество ПЭ в системе может быть больше, чем в SMP-системах с шинной организацией, и может достигать 512, хотя типовые значения - 8-16 процессорных элементов.
                    PVP-системы ориентированы на приложения из таких областей промышленности, как аэрокосмическая, автомобильная, электронная, химическая, энергетическая и т.п. Разработчики PVP-систем предлагают пользователям компиляторы с эффективными средствами автоматической векторизации и распараллеливания программных вычислений. Получаемый объектный код позволяет наилучшим образом использовать потенциал множества векторных процессорных элементов. Кроме того, нужно учитывать, что передача данных в векторном формате происходит на два порядка быстрее, чем в скалярном. Как следствие, затраты времени на взаимодействие между параллельными потоками данных существенно снижаются, что ощутимо сказывается на общей производительности системы.
                    Основными производителями, поддерживающими производство PVP-систем, являются японские фирмы NEC, Fujitsu и Hitachi. Подобную архитектуру имеют системы семейства VPP (Fujitsu). Каждый процессорный элемент системы состоит из векторного и скалярного функциональных устройств, блока памяти и устройства сопряжения. ПЭ связаны коммутатором типа «кроссбар». В зависимости от модели система может включать от 8 до 256 процессорных элементов. В максимальном варианте пиковая производительность ВС достигает 4,9 TFLOPS. Работу системы поддерживает специализированная операционная система, основанная на ОС UNIX.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 43
              - column:
                  name: name
                  value: Cray C90
              - column:
                  name: description
                  value: Система Cray С90 («Cray for the 90s» - ВС для 90-х годов XX в.) была построена в 1991 году, ее максимальное быстродействие достигало 16 GFLOPS. Для формирования данной системы были впервые применены процессоры с быстродействием 1 GFLOPS. Допустимое количество процессоров в конфигурациях ВС - 2, 4, 8 и 16; емкость оперативной памяти - 512 Мбайт...8 Гбайт.
                    Архитектура ВС Cray С90 в целом относится к классу MIMD; это мультипроцессорная система с общей памятью. В ее состав входят. подсистема процессоров, подсистема межпроцессорных взаимодействий, общая память и подсистема ввода-вывода информации.
                    Функциональная структура процессора ВС Cray С90 близка к структуре Cray-1 (композиция секции управления, конвейеров, регистров и сети связей). Конвейеры и регистры предназначаются для обработки и хранения данных трех типов. адресов (A- и B-регистры), скалярных операндов (S- и T-регистры) и векторных операндов (V-регистры). Конвейеры подразделяются на четыре группы. адресные, скалярные, векторные и для операций с плавающей запятой. Последняя группа конвейеров предназначается для выполнения как скалярных, так и векторных команд. Общее число конвейеров составляет 14-16. Регистры трех основных наборов (A, S, V) имеют связи как с конвейерами, так и с оперативной памятью. Регистры B и T играют роль буферных для основных A- и S-регистров.
                    Восемь адресных 32-разрядных A-регистров предназначаются для хранения и вычисления адресов, индексации, указания величины сдвигов и числа итераций циклов и т.д. В Cray С90 64 32-разрядных B-регистров. Восемь скалярных 64-разрядных S-регистров применяются для хранения данных и результатов операций скалярной арифметики; их также можно использовать для хранения элементов векторов данных при векторных вычислениях. В системе 64 64-разрядных T-регистров. Восемь векторных регистров (V-регистры) рассчитаны для хранения 128-компонентных векторов данных, причем каждый компонент представляет собой 64-разрядное слово. Эти регистры используются только для выполнения векторных команд. Наряду с названными имеются также регистр длины вектора (8 разрядов) и регистр маски вектора (128 разрядов). При функционировании процессор способен в каждом такте (каждые 4,1 нс) выдавать результаты двух операций. Если выполняется операция «зацепления» сложения и умножения, то процессор фактически за такт реализует четыре арифметических операции. Следовательно, пиковая производительность процессора достигает почти 1 GFLOPS (109 опер./с).
                    Подсистема межпроцессорных взаимодействий предназначена для организации и реализации передач данных и управляющей информации между процессорами. Это, по сути, композиция общедоступных регистров, в которой выделены одинаковые кластеры (группы). Каждый кластер содержит 8 32-разрядных общедоступных адресных регистров (SB), 8 64-разрядных общедоступных скалярных регистров (ST) и 32 1-разрядных регистра для однобитовых семафоров.
                    Оперативная память Cray С90 является общедоступной для всех процессоров и подсистемы ввода-вывода информации. Каждый процессор имеет доступ к памяти через четыре порта, пропускная способность любого порта составляет два слова за один такт (за 4,1 нс). При этом один из портов всегда связан с подсистемой ввода-вывода и, по крайней мере, еще один из портов всегда выделен под операцию записи. Ячейки памяти способны хранить 80-разрядные слова (64 разряда - для хранения операнда и 16 разрядов - для коррекции ошибок).
                    В максимальной конфигурации память разделена на восемь секций, каждая секция - на восемь подсекций и, наконец, каждая подсекция - на 16 банков. Ячейкам памяти присвоены адреса таким образом, что имеет место их чередование по секциям, подсекциям и банкам. При этом возможны конфликты при одновременном обращении, к какой-либо части памяти из разных портов. Так, при одновременном обращении к одной и той же секции возникает задержка на 1 такт, и при обращениях к одной и той же под¬секции в пределах одной секции задержка варьируется от 1 до 6 тактов. При выборке последовательно расположенных данных или при выборке с любым нечетным шагом конфликтов не возникает.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 44
              - column:
                  name: name
                  value: Cray T90
              - column:
                  name: description
                  value: Вычислительная система Cray Т90 была создана в 1995 г., максимальное быстродействие достигало 64 GFLOPS. Промышленные модели данной ВС. Cray Т94, Cray Т916 и Cray Т932, состояли соответственно из 4, 16 и 32 процессоров. Емкость оперативной памяти ВС Cray Т932 составляла 512 Мбайт...8 Гбайт, скорость обмена информацией с памятью - 800 Мбод.
                    Время процессорного цикла ВС Cray Т90 составляло 2,2 нс, однако за счет конвейеризации процессор имел быстродействие 2 GFLOPS (вычислительные средства с такой производительностью даже в начале 90-х годов XX в. относились к суперкомпьютерам).
                    Система Cray Т90 по своей архитектуре относилась к классу MIMD. Кроме того, в ней была предусмотрена возможность построения макросистем, как объединений нескольких ВС Cray Т90.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 45
              - column:
                  name: name
                  value: Cray X-MP
              - column:
                  name: description
                  value: Cray X-MP - суперкомпьютер на базе модели Cray-1, выпущенный в 1982 году командой разработчиков во главе с инженером Стивеном Ченом. Это был первый многопроцессорный векторный компьютер компании Cray Research. С 1983 по 1985 год Cray X-MP был самым быстрым компьютером в мире.
                    Система Cray Х-МР - первая суперВС - кластер из конвейерных процессоров. Архитектура ВС Cray Х-МР относится к классу MIMD, однако эту систему следует воспринимать как «старшую» модель, совместимую с Cray-1. Образец двухпроцессорной ВС был создан в 1982 году, а четырехпроцессорной - в 1984 году. В системе Cray Х-МР может быть два или четыре процессора, максимальное быстродействие составляет 940 MFLOPS, быстродействие одного процессора - 235 MFLOPS, емкость оперативной МОП-памяти произвольной выборки - 64...128 Мбайт, цена четырехпроцессорной конфигурации ВС - 14,6 млн долл.
                    Компания Cray Research производила также мини-суперкомпьютер Cray XMS, совместимый с Cray Х-МР с воздушным охлаждением.
                    Эффективной областью применения ВС Cray Х-МР являлось, например, моделирование авиакосмических объектов. Задачи этой области допускают расщепления вычислительного процесса на два и четыре самостоятельных процесса. Например, на двух процессорах можно рассчитывать воздушные потоки для каждого из двух крыльев самолета или при составлении прогноза погоды можно обрабатывать данные для каждого из двух полушарий Земли.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 46
              - column:
                  name: name
                  value: Cray Y-MP
              - column:
                  name: description
                  value: Cray Y-MP - суперкомпьютер, вышедший в свет в 1988 году.
                    Вычислительная система Cray Y-MP создана в 1988 году, ее архитектурные характеристики заметно превосходят Cray Х-МР. Количество процессоров в системе Cray Y-MP составляло от одного до восьми, максимальное быстродействие ВС - 2,65 GFLOPS, быстродействие одного процессора - 333 MFLOPS, стандартная и максимальная емкости оперативной памяти - 256 Мбайт и 32 Гбайт.
                    Максимальная производительность 1 GFLOPS (т.е. 109 опер./с над 64-разрядными данными с плавающей запятой) была достигнута в ВС Cray Y-MP в 1989 году.
                    С 1994 года фирма Cray Research начала производить системы Cray J90, совместимые с Cray Y-MP, но обладавшие большими возможностями по «масштабированию» (от 4 до 32 процессоров), более компактные и дешевые (с воздушным охлаждением). Система Cray J90 была наиболее популярной в мире (было выпущено свыше 400 шт.).
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 47
              - column:
                  name: name
                  value: Cray-2
              - column:
                  name: description
                  value: Суперкомпьютер Cray-2, выпускавшийся компанией Cray Research с 1985 года, был самым производительным компьютером своего времени, обогнав по производительности другой суперкомпьютер, Cray X-MP. Пиковая производительность Cray-2 составляла 1,9 Гфлопс.
                    Параллельно-векторная вычислительная система Cray-2 была разработана под руководством Сеймура Крея и построена в 1985 году в корпорации Cray Research. Эта ВС имела четырехпроцессорную конфигурацию и самые лучшие технические характеристики для 80-х годов XX столетия. Так, пиковое быстродействие ВС составляло 1,95 GFLOPS (т.е. 1,95∙109 опер./с над операндами с плавающей запятой, составлявшими векторы) и 250 MIPS (т.е. 2,5∙108 опер./с над скалярными величинами с фиксированной запятой). Емкость оперативной памяти ВС Cray-2 достигала 512 Мбайт...32 Гбайт. При этом цена ВС Cray-2 превышала в 2 раза цену ВС Cray-1.
                    По архитектуре ВС Cray-2 резко отличалась от ВС Cray-1, она имела новый набор команд и новую операционную систему. В ней каждому процессору помимо векторных регистров была придана локальная оперативная память емкостью не менее 16 К 64-разрядных слов.
                    Вычислительная система Cray-2 была построена на быстродействующей элементной базе, что позволило достичь длительности цикла в 4,1 нс (вместо 12,5 нс, как это имело место в Cray-1). Быстродействие одного процессора Cray-2 составляло 488 MFLOPS.
                    Конструкция ВС Cray-2 оригинальна и достаточно компактна, ее можно было разместить в цилиндре с основанием 1,35 м и высотой 1,15 м. Компактность конструкции позволила применить для охлаждения Cray-2 метод полного погружения в инертную жидкость.
                    К сожалению Cray-2 ждала неудача, так как его производительность была ненамного выше, чем у Cray X-MP (но в 10 раз выше, чем у Cray-1). Главным достоинством Cray-2 был большой объем очень быстрой памяти. Когда модули этой памяти были установлены в Cray X-MP, новый суперкомпьютер, вышедший в свет в 1988 году под названием Cray Y-MP, легко обогнал Cray-2.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 48
              - column:
                  name: name
                  value: Cray-3
              - column:
                  name: description
                  value: Система Cray-3 разрабатывалась под руководством Сеймура Крея в фирме Cray Computer. В Cray-3 входило до 16 процессоров, время цикла процессора составляло 2,11 нc, ожидаемое быстродействие 16-процессорной конфигурации ВС - 7,3 GFLOPS. В 1989 году была построена конфигурация Cray-3 с быстродействием 5 GFLOPS и емкостью оперативной памяти до 33 Гбайт.По своей архитектуре ВС Cray-3 была достаточно близка к ВС Cray-2. В качестве элементной базы для системы Cray-3 были использованы арсинид-галиевые интегральные схемы. Эта элементная база была более быстродействующей, но и более дорогой, чем кремниевая.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 49
              - column:
                  name: name
                  value: Cray-4
              - column:
                  name: description
                  value: В фирме Cray Computer Corp. в начале 1994 года Сеймур Крэй начал работу над новым компьютером - Cray-4, который был более простой и надежной машиной, чем Cray-3.
                    Работа над Cray-4 шла успешнее, чем Cray-3. Удалось достигнуть тактовой частоты 1 ГГц (время такта - 1 нс), общая стоимость машины была в пять раз ниже, чем Cray-3.
                    Системы Cray-3 и Cray-4 не имели коммерческого успеха.
        - insert:
            tableName: cs_entity
            columns:
              - column:
                  name: id
                  value: 50
              - column:
                  name: name
                  value: Симметричные мультипроцессорные системы
              - column:
                  name: description
                  value: Симметричные мультипроцессорные системы - система, состоящая из нескольких однородных процессоров и массива общей памяти (обычно из нескольких независимых блоков). Все процессоры имеют доступ к любой точке памяти с одинаковой скоростью. Аппаратно поддерживается когерентность кэшей.
                    Примеры SMP-систем. HP 9000 V-class, N-class; SMP-cервера и рабочие станции на базе процессоров Intel (IBM, HP, Compaq, Dell, ALR, Unisys, DG, Fujitsu и др.).
                    До недавнего времени практически все однопользовательские персональные ВМ и рабочие станции содержали по одному микропроцессору общего назначения. По мере возрастания требований к производительности и снижения стоимости микропроцессоров поставщики вычислительных средств как альтернативу однопроцессорным ВМ стали предлагать симметричные мультипроцессорные вычислительные системы, так называемые SMP-системы (Symmetric MultiProcessor). Это понятие относится как к архитектуре ВС, так и к операционной системе, обслуживающей данную архитектуру.
                    SMP-система состоит из множества процессорных элементов (ПЭ), каждый из которых имеет равноправный доступ к логически единой памяти (физически память обычно строится по блочному принципу). Таким образом, в системе реализована концепция однородного доступа к памяти (UMA). Обычно все ПЭ идентичны, однако допускается использование и различных ПЭ. В последнем случае главное условие - сопоставимая производительность, с тем чтобы при распределении вычислительной нагрузки ее можно было без проблем возложить на любой ПЭ системы. Одинаковые возможности ПЭ и их равноправие при доступе к памяти обусловили термин «симметричная» в названии данного вида ВС.
                    Все процессорные элементы управляются единственным экземпляром операционной системы (ОС), загружаемой в совместно используемую память. ОС планирует распределение выполняемых заданий между процессорными элементами. Для этого ОС выделяет в процессах фрагменты (нити) и образует из этих фрагментов единую очередь. При освобождении какого-либо ПЭ (неважно, какого) ему сразу же передается очередной фрагмент из очереди. Современные ОС обычно поддерживают работу 16 или 32 ПЭ, хотя в некоторых UNIX-подобных операционных системах заложена поддержка 64 процессорных элементов.
                    Хотя технически SMP-системы симметричны, в их работе присутствует небольшой фактор перекоса, который вносит программное обеспечение. На время загрузки системы один из процессоров получает статус ведущего (master). Это не означает, что позже, во время работы какие-то процессоры будут ведомыми - все они в SMP-системе равноправны. Термин «ведущий» лишь указывает, какой из процессоров будет руководить первоначальной загрузкой ВС. В некоторых SMP-системах ведущим назначается ПЭ с наибольшим номером.
                    Типовая SMP- система содержит от двух до 32 идентичных процессоров, в качестве которых обычно выступают недорогие RISC- процессоры. В последнее время наметилась тенденция оснащения SMP-систем также и CISC-процессорами.
                    Каждый процессор снабжен локальной кэш-памятью. Согласованность содержимого кэш-памяти всех процессоров обеспечивается аппаратными средствами. В некоторых SMP- системах проблема когерентности снимается за счет разделяемой кэш- памяти. К сожалению, этот прием технически и экономически оправдан лишь при числе процессоров не большем четырех. Применение разделяемой кэш- памяти сопровождается повышением стоимости и снижением быстродействия.
                    Все процессоры ВС имеют равноправный доступ к разделяемой основной памяти и устройствам ввода/вывода. Такая возможность обеспечивается коммуникационной системой. Обычно процессоры взаимодействуют между собой через основную память (сообщения и информация о состоянии оставляются в области общих данных). В некоторых SMP- системах предусматривается также прямой обмен сигналами между процессорами.
                    Память системы обычно строится по блочному принципу и организована так, что допускается одновременное обращение к разным ее банкам. В некоторых конфигурациях (в дополнение к совместно используемым ресурсам) каждый процессор обладает также собственными дополнительными средствами (локальной основной памятью и каналами ввода/ вывода).


